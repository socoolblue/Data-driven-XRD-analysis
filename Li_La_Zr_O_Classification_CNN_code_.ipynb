{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "\n",
    "#================================================================== Data Split ======================================================\n",
    "\n",
    "import numpy as np\n",
    "import random, shutil, glob\n",
    "\n",
    "num_test_file = 100\n",
    "\n",
    "File_List = glob.glob('file path_total')\n",
    "list_index = list(np.arange(0, len(File_List), 1))\n",
    "random_index = random.sample(list_index,len(File_List))\n",
    "for i in range(len(random_index)):\n",
    "    random_index_ = random_index[i]\n",
    "\n",
    "#=================================================================== Test data set =====================================================    \n",
    "    if i < num_test_file:\n",
    "        shutil.copy('file path_total' %random_index_, 'file path_Test' %random_index_)\n",
    "#============================================================ Training data set + validation data set ==========================================        \n",
    "    if num_test_file <= i:\n",
    "        shutil.copy('file path_total' %random_index_, 'file path_Training' %random_index_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time, glob\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_training_data = 69943\n",
    "num_validation_data = 10000\n",
    "num_test_data = 10000\n",
    "X_length = 4012\n",
    "batch_size_ = 1000\n",
    "n_classes = 21\n",
    "epochs = 1\n",
    "before_gen_num= 0\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    input_data = tf.placeholder(tf.float32, shape=[None, X_length])\n",
    "    input_data_vld = tf.placeholder(tf.float32, shape=[None, X_length])\n",
    "    input_data_tes = tf.placeholder(tf.float32, shape=[None, X_length])\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(input_data).shuffle(num_training_data).repeat().batch(batch_size_)\n",
    "    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)\n",
    "    xy_data = iterator.get_next()\n",
    "    \n",
    "    y_1=tf.cast(xy_data[:,-8], tf.int32)\n",
    "    y_2=tf.cast(xy_data[:,-7], tf.int32)\n",
    "    y_3=tf.cast(xy_data[:,-6], tf.int32)\n",
    "\n",
    "    y_1=tf.one_hot(y_1, n_classes)\n",
    "    y_2=tf.one_hot(y_2, n_classes)\n",
    "    y_3=tf.one_hot(y_3, n_classes)\n",
    "    y_data = tf.add_n([y_1, y_2, y_3])\n",
    "    y_data_ = y_1*tf.reshape(xy_data[:,-5],[-1,1])+y_2*tf.reshape(xy_data[:,-4],[-1,1])+y_3*tf.reshape(xy_data[:,-3],[-1,1])\n",
    "    y_data = tf.cast(y_data, tf.float32)\n",
    "    y_data_ = tf.cast(y_data_, tf.float32)\n",
    "    \n",
    "    X_train, y_train, y_train_, y_train_p, y_train_ind = [xy_data[:,:-11], y_data, y_data_, xy_data[:,-5:-2], xy_data[:,-1:]]\n",
    "    \n",
    "    print(X_train, y_train, y_train_, y_train_p, y_train_ind)\n",
    "    \n",
    "#========================================================================== Validation set ==========================================   \n",
    "    \n",
    "    dataset_vld = tf.data.Dataset.from_tensor_slices(input_data_vld).shuffle(num_validation_data).repeat().batch(batch_size_)\n",
    "    iterator_vld = tf.compat.v1.data.make_initializable_iterator(dataset_vld)\n",
    "    xy_data_vld = iterator_vld.get_next()\n",
    "    \n",
    "    y_1_vld=tf.cast(xy_data_vld[:,-8], tf.int32)\n",
    "    y_2_vld=tf.cast(xy_data_vld[:,-7], tf.int32)\n",
    "    y_3_vld=tf.cast(xy_data_vld[:,-6], tf.int32)\n",
    "\n",
    "    y_1_vld=tf.one_hot(y_1_vld, n_classes)\n",
    "    y_2_vld=tf.one_hot(y_2_vld, n_classes)\n",
    "    y_3_vld=tf.one_hot(y_3_vld, n_classes)\n",
    "    y_data_vld = tf.add_n([y_1_vld, y_2_vld, y_3_vld])\n",
    "    y_data_vld_ = y_1_vld*tf.reshape(xy_data_vld[:,-5],[-1,1])+y_2_vld*tf.reshape(xy_data_vld[:,-4],[-1,1])+y_3_vld*tf.reshape(xy_data_vld[:,-3],[-1,1])\n",
    "    y_data_vld = tf.cast(y_data_vld, tf.float32)\n",
    "    y_data_vld_ = tf.cast(y_data_vld_, tf.float32)\n",
    "    \n",
    "    X_vld, y_vld, y_vld_, y_vld_p, y_vld_ind = [xy_data_vld[:,:-11], y_data_vld, y_data_vld_, xy_data_vld[:,-5:-2], xy_data_vld[:,-1:]]\n",
    "        \n",
    "#========================================================================== Test set ==========================================     \n",
    "    \n",
    "    dataset_tes = tf.data.Dataset.from_tensor_slices(input_data_tes).shuffle(num_test_data).repeat().batch(batch_size_)\n",
    "    iterator_tes = tf.compat.v1.data.make_initializable_iterator(dataset_tes)\n",
    "    xy_data_tes = iterator_tes.get_next()\n",
    "    \n",
    "    y_1_tes=tf.cast(xy_data_tes[:,-8], tf.int32)\n",
    "    y_2_tes=tf.cast(xy_data_tes[:,-7], tf.int32)\n",
    "    y_3_tes=tf.cast(xy_data_tes[:,-6], tf.int32)\n",
    "\n",
    "    y_1_tes=tf.one_hot(y_1_tes, n_classes)\n",
    "    y_2_tes=tf.one_hot(y_2_tes, n_classes)\n",
    "    y_3_tes=tf.one_hot(y_3_tes, n_classes)\n",
    "    y_data_tes = tf.add_n([y_1_tes, y_2_tes, y_3_tes])\n",
    "    y_data_tes_ = y_1_tes*tf.reshape(xy_data_tes[:,-5],[-1,1])+y_2_tes*tf.reshape(xy_data_tes[:,-4],[-1,1])+y_3_tes*tf.reshape(xy_data_tes[:,-3],[-1,1])\n",
    "    y_data_tes = tf.cast(y_data_tes, tf.float32)\n",
    "    y_data_tes_ = tf.cast(y_data_tes_, tf.float32)\n",
    "    \n",
    "    X_tes, y_tes, y_tes_, y_tes_p, y_tes_ind = [xy_data_tes[:,:-11], y_data_tes, y_data_tes_, xy_data_tes[:,-5:-2], xy_data_tes[:,-1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    inputs_ = tf.placeholder(tf.float32, [None, 4001, 1], name = 'inputs')\n",
    "    labels_1 = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_1')\n",
    "    logit_num = tf.placeholder(tf.int32, [None, 3], name = 'logits_Top_3')\n",
    "    label_num = tf.placeholder(tf.int32, [None, 3], name = 'labels_Top_3')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "\n",
    "    #   model architecture(CNN_2)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    conv1 = tf.keras.layers.Conv1D(filters=64, kernel_size=15, strides=2,padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(inputs_)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=10, strides=3, padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3, padding='same')(conv2)\n",
    "    flat = tf.keras.layers.Flatten()(max_pool_2)\n",
    "    flat = tf.keras.layers.Dropout(rate=keep_prob_)(flat)\n",
    "    logits = tf.keras.layers.Dense(units=2000, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(flat)\n",
    "    logits= tf.keras.layers.Dropout(rate=keep_prob_)(logits)\n",
    "    logits_ = tf.keras.layers.Dense(units=500, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(logits)\n",
    "    logits_= tf.keras.layers.Dropout(rate=keep_prob_)(logits_)\n",
    "    logits_2 = tf.keras.layers.Dense(units=n_classes, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(logits_)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #   model architecture(CNN_3)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    conv1 = tf.keras.layers.Conv1D(filters=64, kernel_size=20, strides=1,padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(inputs_)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3, padding='same')(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=15, strides=1, padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(pool_size=3, strides=3, padding='same')(conv2)\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=64, kernel_size=10, strides=2, padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_2)\n",
    "    max_pool_3 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(conv3)\n",
    "    flat = tf.keras.layers.Flatten()(max_pool_3)\n",
    "    flat = tf.keras.layers.Dropout(rate=keep_prob_)(flat)\n",
    "    logits = tf.keras.layers.Dense(units=2500, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(flat)\n",
    "    logits= tf.keras.layers.Dropout(rate=keep_prob_)(logits)\n",
    "    logits_ = tf.keras.layers.Dense(units=1000, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(logits)\n",
    "    logits_= tf.keras.layers.Dropout(rate=keep_prob_)(logits_)\n",
    "    logits_2 = tf.keras.layers.Dense(units=n_classes, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(logits_)\n",
    "    \n",
    "    '''\n",
    "    #   model architecture(CNN_4)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    conv1 = tf.keras.layers.Conv1D(filters=64, kernel_size=25, strides=1,padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(inputs_)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=20, strides=1, padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(conv2)\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=64, kernel_size=15, strides=1, padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_2)\n",
    "    max_pool_3 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(conv3)\n",
    "    conv4 = tf.keras.layers.Conv1D(filters=64, kernel_size=10, strides=2,padding='same', \n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_3)\n",
    "    max_pool_4 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(conv4) \n",
    "    flat = tf.keras.layers.Flatten()(max_pool_4)\n",
    "    flat = tf.keras.layers.Dropout(rate=keep_prob_)(flat)\n",
    "    logits = tf.keras.layers.Dense(units=2500, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(flat)\n",
    "    logits= tf.keras.layers.Dropout(rate=keep_prob_)(logits)\n",
    "    logits_ = tf.keras.layers.Dense(units=1000, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(logits)\n",
    "    logits_= tf.keras.layers.Dropout(rate=keep_prob_)(logits_)\n",
    "    logits_2 = tf.keras.layers.Dense(units=n_classes, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(logits_)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #   model architecture(CNN_5)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    conv1 = tf.keras.layers.Conv1D(filters=64, kernel_size=30, strides=1,padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(inputs_)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=25, strides=1, padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(conv2)\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=64, kernel_size=20, strides=1, padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_2)\n",
    "    max_pool_3 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(conv3)\n",
    "    conv4 = tf.keras.layers.Conv1D(filters=64, kernel_size=15, strides=1,padding='same', \n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_3)\n",
    "    max_pool_4 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(conv4) \n",
    "    conv5 = tf.keras.layers.Conv1D(filters=64, kernel_size=10, strides=1, padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_4)\n",
    "    max_pool_5 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(conv5)\n",
    "    flat = tf.keras.layers.Flatten()(max_pool_5)\n",
    "    flat = tf.keras.layers.Dropout(rate=keep_prob_)(flat)\n",
    "    logits = tf.keras.layers.Dense(units=2500, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(flat)\n",
    "    logits= tf.keras.layers.Dropout(rate=keep_prob_)(logits)\n",
    "    logits_ = tf.keras.layers.Dense(units=1000, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(logits)\n",
    "    logits_= tf.keras.layers.Dropout(rate=keep_prob_)(logits_)\n",
    "    logits_2 = tf.keras.layers.Dense(units=n_classes, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(logits_)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #   model architecture(CNN_6)\n",
    "    \n",
    "    \n",
    "    conv1 = tf.keras.layers.Conv1D(filters=64, kernel_size=35, strides=1,padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(inputs_)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=30, strides=1, padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(conv2)\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=64, kernel_size=25, strides=1, padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_2)\n",
    "    max_pool_3 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(conv3)\n",
    "    conv4 = tf.keras.layers.Conv1D(filters=64, kernel_size=20, strides=1,padding='same', \n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_3)\n",
    "    max_pool_4 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(conv4) \n",
    "    conv5 = tf.keras.layers.Conv1D(filters=64, kernel_size=15, strides=1, padding='same',\n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_4)\n",
    "    max_pool_5 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(conv5)\n",
    "    conv6 = tf.keras.layers.Conv1D(filters=64, kernel_size=10, strides=1,padding='same', \n",
    "                                   kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform(), activation = tf.nn.relu)(max_pool_5)\n",
    "    max_pool_6 = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(conv6) \n",
    "    flat = tf.keras.layers.Flatten()(max_pool_6)\n",
    "    flat = tf.keras.layers.Dropout(rate=keep_prob_)(flat)\n",
    "    logits = tf.keras.layers.Dense(units=2500, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(flat)\n",
    "    logits= tf.keras.layers.Dropout(rate=keep_prob_)(logits)\n",
    "    logits_ = tf.keras.layers.Dense(units=1000, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(logits)\n",
    "    logits_= tf.keras.layers.Dropout(rate=keep_prob_)(logits_)\n",
    "    logits_2 = tf.keras.layers.Dense(units=n_classes, kernel_initializer=tf.compat.v1.keras.initializers.glorot_uniform())(logits_)\n",
    "    \n",
    "    \n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_2, labels=labels_1))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n",
    "    correct_pred = tf.math.equal(logit_num, label_num)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        train_result_list=[]\n",
    "        train_result_list_1=[]\n",
    "        \n",
    "        validation_result_list=[]\n",
    "        validation_result_list_1=[]\n",
    "        file_list_total= glob.glob('/file path/*.csv')\n",
    "        print(file_list_total)\n",
    "        arrays = [np.loadtxt(file_name, delimiter=',', dtype=np.float32) for file_name in file_list_total]\n",
    "        stack = np.concatenate(arrays, axis=0)\n",
    "        np.random.shuffle(stack)\n",
    "        data = stack[:-num_validation_data]\n",
    "        data_vld = stack[-num_validation_data:]\n",
    "                \n",
    "        sess.run(iterator.initializer, feed_dict = {input_data:data})\n",
    "        sess.run(iterator_vld.initializer, feed_dict = {input_data_vld:data_vld})    \n",
    "        \n",
    "        iteration = 1\n",
    "        \n",
    "        if e+before_gen_num ==0:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "        else:\n",
    "            saver.restore(sess, 'file path')\n",
    "\n",
    "        for i in range(70):     \n",
    "            sum_ = 0\n",
    "            sum_1 = 0\n",
    "            X_tr, y_tr, y_tr_, y_tr_p, y_ind = sess.run([X_train, y_train, y_train_, y_train_p, y_train_ind])\n",
    "            X_tr= np.reshape(X_tr, (-1, 4001, 1))\n",
    "            feed = {inputs_ : X_tr, labels_1 : y_tr, keep_prob_ : 0.5, learning_rate_ : learning_rate}\n",
    "            loss, _ , logit = sess.run([cost, optimizer, logits_2], feed_dict = feed)\n",
    "            y_lab = np.empty([batch_size_, 3])\n",
    "            y_logit = np.empty([batch_size_, 3])\n",
    "            \n",
    "            for i in range(batch_size_):\n",
    "                if y_ind[i,0] == 2:\n",
    "                    y_lab[i]=np.argsort(y_tr_[i])[-3:]\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    y_logit[i]=np.argsort(logit[i])[-3:]\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1= sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1\n",
    "                elif y_ind[i,0] == 1:\n",
    "                    z=np.argsort(y_tr_[i])[-2:]\n",
    "                    y_lab[i]=np.append(z, [-1])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-2:]\n",
    "                    y_logit[i]=np.append(z_, [-1])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1 = sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1         \n",
    "                elif y_ind[i,0] == 0:\n",
    "                    z=np.argsort(y_tr_[i])[-1:]\n",
    "                    y_lab[i]=np.append(z, [-1,-2])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-1:]\n",
    "                    y_logit[i]=np.append(z_, [-1,-2])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1 = sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1\n",
    "            acc_=sum_/len(y_lab)\n",
    "            acc_1=sum_1/(len(y_lab)*y_lab.shape[1])\n",
    "            \n",
    "            result=['Epoch:' , e, 'iteration:', iteration, 'Train_loss:', loss, 'Train_sample_acc:', acc_]\n",
    "            result_1=['Epoch:' , e, 'iteration:', iteration, 'Train_loss:', loss, 'Train_constituent_acc:', acc_1]\n",
    "            \n",
    "            train_result_list.append(result)\n",
    "            train_result_list_1.append(result_1)\n",
    "            \n",
    "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                   \"Iteration: {:d}\".format(iteration),\n",
    "                   \"Train loss: {:6f}\".format(loss),\n",
    "                   \"Train sample acc: {:.6f}\".format(acc_),\n",
    "                   \"Train constituent acc_1: {:.6f}\".format(acc_1))\n",
    "\n",
    "###================================================================ VALIDATION =====================================            \n",
    "            \n",
    "            if (iteration %5 == 0):\n",
    "                sum_ = 0\n",
    "                sum_1 = 0\n",
    "                X_vd, y_vd, y_vd_, y_vd_p, y_ind_vd = sess.run([X_vld, y_vld, y_vld_, y_vld_p, y_vld_ind])\n",
    "                X_vd= np.reshape(X_vd, (-1, 4001, 1))\n",
    "                feed = {inputs_ : X_vd, labels_1 : y_vd, keep_prob_ : 1.0, learning_rate_ : learning_rate}\n",
    "                loss_vd,  logit_vd = sess.run([cost, logits_2], feed_dict = feed)\n",
    "                y_lab_vd = np.empty([batch_size_, 3])\n",
    "                y_logit_vd = np.empty([batch_size_, 3])\n",
    "                for i in range(batch_size_):\n",
    "                    if y_ind_vd[i,0] == 2:\n",
    "                        y_lab_vd[i]=np.argsort(y_vd_[i])[-3:]\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])\n",
    "                        y_logit_vd[i]=np.argsort(logit_vd[i])[-3:]\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                        b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                        sum_1= sum_1+b1\n",
    "                        if b1 == 3:\n",
    "                            sum_+=1\n",
    "                    elif y_ind_vd[i,0] == 1:\n",
    "                        z=np.argsort(y_vd_[i])[-2:]\n",
    "                        y_lab_vd[i]=np.append(z, [-1])\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])                    \n",
    "                        z_=np.argsort(logit_vd[i])[-2:]\n",
    "                        y_logit_vd[i]=np.append(z_, [-1])\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                        b1 = len(list(set(y_lab_vd[i]).intersection(y_logit_vd[i])))\n",
    "                        sum_1= sum_1+b1\n",
    "                        if b1 == 3:\n",
    "                            sum_+=1\n",
    "                    elif y_ind_vd[i,0] == 0:\n",
    "                        z=np.argsort(y_vd_[i])[-1:]\n",
    "                        y_lab_vd[i]=np.append(z, [-1,-2])\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])\n",
    "                        z_=np.argsort(logit_vd[i])[-1:]\n",
    "                        y_logit_vd[i]=np.append(z_, [-1,-2])\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                        b1 = len(list(set(y_lab_vd[i]).intersection(y_logit_vd[i])))\n",
    "                        sum_1= sum_1+b1\n",
    "                        if b1 == 3:\n",
    "                            sum_+=1\n",
    "                acc_vd_=sum_/len(y_lab_vd)            \n",
    "                acc_vd_1=sum_1/(len(y_lab_vd)*y_lab_vd.shape[1])\n",
    "                result_validation=['Epoch:' , e, 'iteration:', iteration, 'Validation_loss:', loss_vd, 'Validation_sample_acc:', acc_vd_]\n",
    "                result_validation_1=['Epoch:' , e, 'iteration:', iteration, 'Validation_loss:', loss_vd, 'Validation_constituent_acc:', acc_vd_1]\n",
    "                validation_result_list.append(result_validation)\n",
    "                validation_result_list_1.append(result_validation_1)\n",
    "                \n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                        \"Iteration: {:d}\".format(iteration),\n",
    "                        \"Validation loss: {:6f}\".format(loss_vd),\n",
    "                        \"Validation sample acc: {:.6f}\".format(acc_vd_),\n",
    "                         \"Validation constituent acc_1: {:.6f}\".format(acc_vd_1))\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        saver.save(sess,'file path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code\n",
    "\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_acc_1=[]\n",
    "iteration = 1\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_list_tes= glob.glob('/file path/*.csv')\n",
    "    arrays_tes = [np.loadtxt(file_name_tes, delimiter=',', dtype=np.float32) for file_name_tes in file_list_tes]\n",
    "    data_tes = np.concatenate(arrays_tes, axis=0)\n",
    "    sess.run(iterator_tes.initializer, feed_dict = {input_data_tes:data_tes})\n",
    "    saver.restore(sess, 'file path')\n",
    "    \n",
    "    for i in  range(10):\n",
    "        sum_ = 0\n",
    "        sum_1 = 0\n",
    "        X_te, y_te, y_te_, y_te_p, y_ind = sess.run([X_tes, y_tes, y_tes_, y_tes_p, y_tes_ind])\n",
    "        X_te = np.reshape(X_te, (-1, 4001, 1))\n",
    "        feed = {inputs_ : X_te, labels_1 : y_te, keep_prob_ : 1.0, learning_rate_ : learning_rate}\n",
    "        batch_loss, logit = sess.run([cost, logits_2], feed_dict = feed)            \n",
    "        y_lab = np.empty([batch_size_, 3])\n",
    "        y_logit = np.empty([batch_size_, 3])\n",
    "        ann_result = np.empty([batch_size_, 2])\n",
    "        \n",
    "        for i in range(batch_size_):\n",
    "            if y_ind[i,0] == 2:\n",
    "                y_lab[i]=np.argsort(y_te_[i])[-3:]\n",
    "                y_lab[i]=np.sort(y_lab[i])\n",
    "                y_logit[i]=np.argsort(logit[i])[-3:]\n",
    "                y_logit[i]=np.sort(y_logit[i])\n",
    "                b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                sum_1= sum_1+b1\n",
    "                if b1 == 3:\n",
    "                    sum_+=1\n",
    "            elif y_ind[i,0] == 1:\n",
    "                z=np.argsort(y_te_[i])[-2:]\n",
    "                y_lab[i]=np.append(z, [-1])\n",
    "                y_lab[i]=np.sort(y_lab[i])\n",
    "                z_=np.argsort(logit[i])[-2:]\n",
    "                y_logit[i]=np.append(z_, [-1])\n",
    "                y_logit[i]=np.sort(y_logit[i])\n",
    "                y_te_p[i] = np.sort(y_te_p[i])\n",
    "                b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                sum_1= sum_1+b1\n",
    "                if b1 == 3:\n",
    "                    sum_+=1\n",
    "            elif y_ind[i,0] == 0:\n",
    "                z=np.argsort(y_te_[i])[-1:]\n",
    "                y_lab[i]=np.append(z, [-1,-2])\n",
    "                y_lab[i]=np.sort(y_lab[i])\n",
    "                z_=np.argsort(logit[i])[-1:]\n",
    "                y_logit[i]=np.append(z_, [-1,-2])\n",
    "                y_logit[i]=np.sort(y_logit[i])\n",
    "                y_te_p[i] = np.sort(y_te_p[i])\n",
    "                b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                sum_1= sum_1+b1\n",
    "                if b1 == 3:\n",
    "                    sum_+=1\n",
    "            else:\n",
    "                print('Something Wrong happened!!!')\n",
    "               \n",
    "        '''\n",
    "        The following code can also be used for test.\n",
    "        The number of positive nodes on the output layer before the sigmoid-cross-entropy loss function can be used as being indicative of whether \n",
    "        the sample to be tested is unary, binary or ternary.\n",
    "        k is the adjustable threshold parameter. Normally k=0, then almost the same test accuracy as the original code above can be obtained.\n",
    "        If k>0, the test accuracy will improve especially for real-word-data test.\n",
    "        '''        \n",
    "        \n",
    "        '''\n",
    "        k=1\n",
    "        if k!=0:\n",
    "            sum_ = 0\n",
    "            sum_1 = 0\n",
    "            for i in range(batch_size_):\n",
    "                if np.count_nonzero(logit[i] > 1.0) >= 3:\n",
    "                    y_lab[i]=np.argsort(y_te_[i])[-3:]\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    y_logit[i]=np.argsort(logit[i])[-3:]\n",
    "                    y_logit[i]=np.sort(y_logit[i])                     \n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1= sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1 \n",
    "                elif np.count_nonzero(logit[i] > 1.0) == 2:\n",
    "                    z=np.argsort(y_te_[i])[-2:]\n",
    "                    y_lab[i]=np.append(z, [-1])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-2:]\n",
    "                    y_logit[i]=np.append(z_, [-1])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1= sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1\n",
    "                elif np.count_nonzero(logit[i] > 1.0) == 1:\n",
    "                    z=np.argsort(y_te_[i])[-1:]\n",
    "                    y_lab[i]=np.append(z, [-1,-2])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-1:]\n",
    "                    y_logit[i]=np.append(z_, [-1,-2])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1= sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1\n",
    "                else:\n",
    "                    print('Something Wrong happened!!!')\n",
    "                    print(np.sort(logit[i])[::-1])\n",
    "                    '''\n",
    "                \n",
    "        acc_te_=sum_/len(y_lab)            \n",
    "        acc_te_1=sum_1/(len(y_lab)*y_lab.shape[1])\n",
    "        test_loss.append(batch_loss)\n",
    "        test_acc.append(acc_te_)\n",
    "        test_acc_1.append(acc_te_1)\n",
    "        \n",
    "        print(\"Iteration: {:d}\".format(iteration),\n",
    "                  \"Test loss: {:6f}\".format(batch_loss),\n",
    "                  \"Test sample-based acc: {:.6f}\".format(acc_te_),\n",
    "                  \"Test constituent-based acc_1: {:.6f}\".format(acc_te_1))\n",
    "        iteration += 1\n",
    "    print(\"Average Test loss: {:.6f}\".format(np.mean(test_loss)))\n",
    "    print(\"Average Test sample_based accuracy: {:.6f}\".format(np.mean(test_acc)))\n",
    "    print(\"Average Test constituent_based accuracy: {:.6f}\".format(np.mean(test_acc_1)))\n",
    "    \n",
    "####################################################################End of Test ##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
