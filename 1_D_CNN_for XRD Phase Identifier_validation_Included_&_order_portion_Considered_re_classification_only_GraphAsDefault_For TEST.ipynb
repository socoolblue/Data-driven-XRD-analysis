{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XRD Deep learning CNN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "# hyper parameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_files = 3624 \n",
    "num_files_te = 1000\n",
    "\n",
    "vld_num_files = 800\n",
    "X_length = 4508\n",
    "\n",
    "batch_size_ = 1000\n",
    "\n",
    "n_classes = 57\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer([(\"D:\\\\AL_DATA\\\\data_4501_Sr_Al_Li_O_003\\\\Test\\\\%d.csv\" % i) for i in range(num_files_te)], shuffle=False, name='filename_queue')\n",
    "\n",
    "    reader_Test = tf.TextLineReader()\n",
    "    key, value_Test = reader_Test.read(filename_queue)\n",
    "\n",
    "    record_defaults = [[0] for _ in range(X_length)]\n",
    "    record_defaults = [tf.constant([0], dtype=tf.float32) for _ in range(X_length)]\n",
    "\n",
    "    xy_data_Test = tf.decode_csv(value_Test, record_defaults=record_defaults)\n",
    "    xy_data_Test = tf.stack(xy_data_Test)\n",
    "\n",
    "    y_1=tf.cast(xy_data_Test[-7], tf.int32)\n",
    "    y_2=tf.cast(xy_data_Test[-6], tf.int32)\n",
    "    y_3=tf.cast(xy_data_Test[-5], tf.int32)\n",
    "    print(y_1, y_2,y_3)\n",
    "\n",
    "    y_1_Test=tf.one_hot(y_1, n_classes)\n",
    "    y_2_Test=tf.one_hot(y_2, n_classes)\n",
    "    y_3_Test=tf.one_hot(y_3, n_classes)\n",
    "    y_data_Test = y_1_Test + y_2_Test + y_3_Test\n",
    "    y_data_Test_ = y_1_Test*xy_data_Test[-4] + y_2_Test*xy_data_Test[-3] + y_3_Test*xy_data_Test[-4]\n",
    "    y_data_Test = tf.to_float(y_data_Test)\n",
    "    y_data_Test_ = tf.to_float(y_data_Test_)\n",
    "\n",
    "    X_Test, y_Test, y_Test_, y_Test_p, y_Test_ind = tf.train.batch([xy_data_Test[:-7], y_data_Test, y_data_Test_, \n",
    "                                                                xy_data_Test[-4:-1], xy_data_Test[-1:]], batch_size = batch_size_)\n",
    "    #X_tr, X_vld = tf.split(X_train, [split_size_tr, split_size_vld], 0)\n",
    "    #y_tr, y_vld = tf.split(y_train, [split_size_tr, split_size_vld], 0)\n",
    "    #y_tr_p, y_vld_p = tf.split(y_train_p, [split_size_tr, split_size_vld], 0)\n",
    "    print(X_Test, y_Test,y_Test_, y_Test_p, y_Test_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    \n",
    "    inputs_ = tf.placeholder(tf.float32, [None, 4501, 1], name = 'inputs')\n",
    "    labels_1 = tf.placeholder(tf.float32, [None, 57], name = 'labels_1')\n",
    "    #labels_2 = tf.placeholder(tf.float32, [None, 57], name = 'labels_2')\n",
    "    logit_num = tf.placeholder(tf.int32, [None, 3], name = 'logits_Top_3')\n",
    "    label_num = tf.placeholder(tf.int32, [None, 3], name = 'labels_Top_3')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "\n",
    "\n",
    "    print(inputs_.shape)\n",
    "    # (batch, 4501, 1) --> (batch, ??, 521)\n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=50, strides=1,padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=3, padding='same')\n",
    "    print(max_pool_1.shape)\n",
    "    \n",
    "    # (batch, ??, 512) --> (batch, ??, 256)\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=25, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=3, padding='same')\n",
    "    print(max_pool_2.shape)\n",
    "    # (batch, ??, 256) --> (batch, ??, 128)\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=10, strides=2, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=1, strides=2, padding='same')\n",
    "    print(max_pool_3.shape)\n",
    "    ## (batch, ??, 128) --> (batch, ??, 64)\n",
    "    #conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=256, kernel_size=5, strides=1,padding='same', initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    #max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=1, padding='same') \n",
    "    #print(max_pool_4.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Flatten and add dropout\n",
    "    flat = tf.reshape(max_pool_3, (-1, 126*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "\n",
    "    print(flat.shape) \n",
    "\n",
    "\n",
    "    # Predictions\n",
    "    logits = tf.layers.dense(flat, 2000, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 1000,  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_1 = tf.layers.dense(logits_, n_classes, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "\n",
    "    #logits_2 = tf.layers.dense(flat, 3)\n",
    " \n",
    "\n",
    "    # Cost function and optimizer\n",
    " \n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_1, labels=labels_1))\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_1, labels=labels_1))\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits_1, labels=labels_1))\n",
    "    #cost = tf.reduce_mean(tf.abs(logits_1 - labels_2))\n",
    "    #cost = cost_1 + cost_2\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n",
    "\n",
    "    # Accuracy\n",
    "\n",
    "    #tf.cast(tf.greater(prediction, threshold), tf.int64)\n",
    "\n",
    "    correct_pred = tf.equal(logit_num, label_num)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Test \n",
    "\n",
    "\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('D:\\\\AL_DATA\\\\data_4501_Sr_Al_Li_O_003\\\\checkpoints-cnn'))\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    for i in  range(100):\n",
    "\n",
    "        X_te, y_te, y_te_, y_te_p, y_ind = sess.run([X_Test, y_Test, y_Test_, y_Test_p, y_Test_ind])\n",
    "        X_te = np.reshape(X_te, (-1, 4501, 1))\n",
    "\n",
    "        feed = {inputs_ : X_te, labels_1 : y_te, keep_prob_ : 1, learning_rate_ : learning_rate}\n",
    "            \n",
    "        # Loss\n",
    "        batch_loss, logit = sess.run([cost, logits_1], feed_dict = feed)            \n",
    "            \n",
    "\n",
    "        \n",
    "        y_lab = np.empty([batch_size_, 3])\n",
    "        y_logit = np.empty([batch_size_, 3])\n",
    "            \n",
    "        for i in range(batch_size_):\n",
    "            if y_ind[i,0] == 2:\n",
    "                    \n",
    "                y_lab[i]=np.argsort(y_te_[i])[-3:]\n",
    "                y_lab[i]=np.sort(y_lab[i])\n",
    "                    \n",
    "                y_logit[i]=np.argsort(logit[i])[-3:]\n",
    "                y_logit[i]=np.sort(y_logit[i])\n",
    "                    \n",
    "            elif y_ind[i,0] == 1:\n",
    "                    \n",
    "                z=np.argsort(y_te_[i])[-2:]\n",
    "                y_lab[i]=np.append(z, [0])\n",
    "                y_lab[i]=np.sort(y_lab[i])\n",
    "                    \n",
    "                z_=np.argsort(logit[i])[-2:]\n",
    "                y_logit[i]=np.append(z_, [0])\n",
    "                y_logit[i]=np.sort(y_logit[i])\n",
    "                    \n",
    "            elif y_ind[i,0] == 0:\n",
    "                    \n",
    "                z=np.argsort(y_te_[i])[-1:]\n",
    "                y_lab[i]=np.append(z, [0,0])\n",
    "                y_lab[i]=np.sort(y_lab[i])\n",
    "                    \n",
    "                z_=np.argsort(logit[i])[-1:]\n",
    "                y_logit[i]=np.append(z_, [0,0])\n",
    "                y_logit[i]=np.sort(y_logit[i])\n",
    "            else:\n",
    "                print('Something Wrong happened!!!')            \n",
    " \n",
    "        feed = {logit_num : y_logit, label_num: y_lab}\n",
    "                 \n",
    "        batch_acc = sess.run(accuracy, feed_dict=feed)\n",
    "        test_loss.append(batch_loss)\n",
    "        test_acc.append(batch_acc)\n",
    "        \n",
    "        print(\"Test loss:\", batch_loss, \"Test accuracy:\", batch_acc)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)        \n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))\n",
    "    print(\"Test loss: {:.6f}\".format(np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a=y_vd[0]\n",
    "\n",
    "b=np.random.rand(57)*100\n",
    "b[21]=120\n",
    "b[8]=110\n",
    "b[55] =100\n",
    "\n",
    "#print(a)\n",
    "print(b)\n",
    "\n",
    "#r=a[np.argsort(a)[-3:]]\n",
    "#z=np.argsort(a)[-3:]\n",
    "#z=np.append(z, [0])\n",
    "\n",
    "x=b[np.argsort(b)[-3:]]\n",
    "y=np.argsort(b)[-3:]\n",
    "#y=np.append(y, [0])\n",
    "\n",
    "#print(r,z)\n",
    "print(x,y)\n",
    "print(np.sort(y.flat))\n",
    "#print(np.sort(y.flat), np.sort(z.flat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
