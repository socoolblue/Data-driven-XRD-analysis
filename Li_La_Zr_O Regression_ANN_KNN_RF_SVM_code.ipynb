{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#artificial_neural_network_regression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time, math\n",
    "\n",
    "def regression_binary_model(file_name,file_num):\n",
    "    from sklearn.metrics import r2_score\n",
    "    tf.set_random_seed(777)\n",
    "    np.random.seed(777)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        xy = np.loadtxt(file_name, delimiter=',', dtype=np.float32)\n",
    "        xy_data=xy.reshape(-1,4012)\n",
    "        np.random.shuffle(xy_data)\n",
    "        x_data_training = xy_data[:2700, :-11]\n",
    "        y_data_training = xy_data[:2700, -5:-3]\n",
    "        x_data_test = xy_data[2700:, :-11]\n",
    "        y_data_test = xy_data[2700:, -5:-3]\n",
    "        X = tf.placeholder(tf.float32, shape=[None, 4001])\n",
    "        Y = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        W1 = tf.get_variable(\"weight1\", shape=[4001, 2000], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W2 = tf.get_variable(\"weight2\", shape=[2000, 2] , initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b1 = tf.Variable(tf.random_normal([2000]), name='bias2')\n",
    "        b2 = tf.Variable(tf.random_normal([2]), name='bias3')\n",
    "        L2 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "        hypothesis = tf.matmul(L2, W2) + b2\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "        train_op  = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    def run_train(sess, train_x, train_y):\n",
    "        print(\"\\nStart training\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        batch_size = 100##100\n",
    "        for epoch in range(200):##200\n",
    "            total_batch = int(train_x.shape[0] / batch_size)\n",
    "            for i in range(total_batch):\n",
    "                batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "                batch_y = train_y[i*batch_size:(i+1)*batch_size]\n",
    "                _, c = sess.run([train_op, cost], feed_dict={X: batch_x, Y: batch_y})\n",
    "                print(\"Epoch #%d step=%d cost=%f\" % (epoch, i, c))\n",
    "    def cross_validate(sess, split_size=5):\n",
    "        results = []\n",
    "        kf = KFold(n_splits=split_size)\n",
    "        for train_idx, val_idx in kf.split(x_data_training, y_data_training):\n",
    "            train_x = x_data_training[train_idx]\n",
    "            train_y = y_data_training[train_idx]\n",
    "            val_x = x_data_training[val_idx]\n",
    "            val_y = y_data_training[val_idx]\n",
    "            run_train(sess, train_x, train_y)\n",
    "            results.append(sess.run(cost, feed_dict={X: val_x, Y: val_y}))\n",
    "        return results\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        test_cost_value=[]\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        result = cross_validate(sess)\n",
    "        print(\"Cross-validation result: %s\" % result)\n",
    "        print(\"Test cost: %f\" % sess.run(cost, feed_dict={X: x_data_test, Y: y_data_test}))\n",
    "        test_cost= sess.run(cost, feed_dict={X: x_data_test, Y: y_data_test})\n",
    "        y_predict = sess.run(hypothesis, feed_dict={X: x_data_test})\n",
    "        r2_score= r2_score(y_data_test, y_predict,multioutput='variance_weighted')\n",
    "        test_cost_value.append(test_cost)\n",
    "        test_cost_value.append(r2_score)\n",
    "        result.extend(test_cost_value)  \n",
    "        result=np.array(result).reshape(-1,len(result))\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, 'file path')\n",
    "        np.savetxt('file path' ,result, delimiter=',',fmt='%4f')\n",
    "\n",
    "def regression_ternary_model(file_name,file_num):\n",
    "    from sklearn.metrics import r2_score\n",
    "    tf.set_random_seed(777)\n",
    "    np.random.seed(777)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        xy = np.loadtxt(file_name, delimiter=',', dtype=np.float32)\n",
    "        xy_data=xy.reshape(-1,4012)\n",
    "        np.random.shuffle(xy_data)\n",
    "        x_data_training = xy_data[:9000, :-11]\n",
    "        y_data_training = xy_data[:9000, -5:-2]\n",
    "        x_data_test = xy_data[9000:, :-11]\n",
    "        y_data_test = xy_data[9000:, -5:-2]\n",
    "        X = tf.placeholder(tf.float32, shape=[None, 4001])\n",
    "        Y = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "        W1 = tf.get_variable(\"weight1\", shape=[4001, 2000], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        W2 = tf.get_variable(\"weight2\", shape=[2000, 3] , initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b1 = tf.Variable(tf.random_normal([2000]), name='bias2')\n",
    "        b2 = tf.Variable(tf.random_normal([3]), name='bias3')\n",
    "        L2 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "        hypothesis = tf.matmul(L2, W2) + b2\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "        train_op  = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    def run_train(sess, train_x, train_y):\n",
    "        print(\"\\nStart training\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        batch_size = 100##100\n",
    "        for epoch in range(200):##200\n",
    "            total_batch = int(train_x.shape[0] / batch_size)\n",
    "            for i in range(total_batch):\n",
    "                batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "                batch_y = train_y[i*batch_size:(i+1)*batch_size]\n",
    "                _, c = sess.run([train_op, cost], feed_dict={X: batch_x, Y: batch_y})\n",
    "                print(\"Epoch #%d step=%d cost=%f\" % (epoch, i, c))\n",
    "    def cross_validate(sess, split_size=5):\n",
    "        results = []\n",
    "        kf = KFold(n_splits=split_size)\n",
    "        for train_idx, val_idx in kf.split(x_data_training, y_data_training):\n",
    "            train_x = x_data_training[train_idx]\n",
    "            train_y = y_data_training[train_idx]\n",
    "            val_x = x_data_training[val_idx]\n",
    "            val_y = y_data_training[val_idx]\n",
    "            run_train(sess, train_x, train_y)\n",
    "            results.append(sess.run(cost, feed_dict={X: val_x, Y: val_y}))\n",
    "        return results\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        test_cost_value=[]\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        result = cross_validate(sess)\n",
    "        print(\"Cross-validation result: %s\" % result)\n",
    "        print(\"Test cost: %f\" % sess.run(cost, feed_dict={X: x_data_test, Y: y_data_test}))\n",
    "        test_cost= sess.run(cost, feed_dict={X: x_data_test, Y: y_data_test})\n",
    "        y_predict = sess.run(hypothesis, feed_dict={X: x_data_test})\n",
    "        r2_score= r2_score(y_data_test, y_predict,multioutput='variance_weighted')\n",
    "        test_cost_value.append(test_cost)\n",
    "        test_cost_value.append(r2_score)\n",
    "        result.extend(test_cost_value)\n",
    "        result=np.array(result).reshape(-1,len(result))\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, 'file path')\n",
    "        np.savetxt('file path' ,result, delimiter=',',fmt='%4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k_nearest_neighbors_regression\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def knn_regression_binary_model(file_name,file_num):\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    tf.set_random_seed(777)  # for reproducibility\n",
    "    np.random.seed(777)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        xy = np.loadtxt(file_name, delimiter=',', dtype=np.float32)\n",
    "        xy_data=xy.reshape(-1,4012) \n",
    "        x_data_training = xy_data[:2700, :-11]\n",
    "        y_data_training = xy_data[:2700, -5:-3]\n",
    "        x_data_test = xy_data[2700:, :-11]\n",
    "        y_data_test = xy_data[2700:, -5:-3]\n",
    "        knn_reg = KNeighborsRegressor(n_neighbors=3, n_jobs=-1)\n",
    "        knn_reg.fit(x_data_training, y_data_training)\n",
    "        y_pred=knn_reg.predict(x_data_test)\n",
    "        mse=mean_squared_error(y_data_test, y_pred, multioutput='uniform_average')\n",
    "        r2=r2_score(y_data_test, y_pred, multioutput='variance_weighted')\n",
    "        result=np.array([mse,r2]).reshape(-1,2)\n",
    "        np.savetxt('file path' ,result, delimiter=',',fmt='%4f')\n",
    "\n",
    "def knn_regression_ternary_model(file_name,file_num):\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    tf.set_random_seed(777)  # for reproducibility\n",
    "    np.random.seed(777)\n",
    "    graph = tf.Graph()  \n",
    "    with graph.as_default():\n",
    "        xy = np.loadtxt(file_name, delimiter=',', dtype=np.float32)\n",
    "        xy_data=xy.reshape(-1,4012) \n",
    "        x_data_training = xy_data[:9000, :-11]\n",
    "        y_data_training = xy_data[:9000, -5:-2]\n",
    "        x_data_test = xy_data[9000:, :-11]\n",
    "        y_data_test = xy_data[9000:, -5:-2]\n",
    "        knn_reg = KNeighborsRegressor(n_neighbors=3, n_jobs=-1)\n",
    "        knn_reg.fit(x_data_training, y_data_training)\n",
    "        y_pred=knn_reg.predict(x_data_test)\n",
    "        mse=mean_squared_error(y_data_test, y_pred, multioutput='uniform_average')\n",
    "        r2=r2_score(y_data_test, y_pred, multioutput='variance_weighted')\n",
    "        result=np.array([mse,r2]).reshape(-1,2)\n",
    "        np.savetxt('file path' ,result, delimiter=',',fmt='%4f')\n",
    "        print(result)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomforest_regression  \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "def rf_regression_binary_model(file_name,file_num):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    tf.set_random_seed(777)  # for reproducibility\n",
    "    np.random.seed(777)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        xy = np.loadtxt(file_name, delimiter=',', dtype=np.float32)\n",
    "        xy_data=xy.reshape(-1,4012) \n",
    "        x_data_training = xy_data[:2700, :-11]\n",
    "        y_data_training = xy_data[:2700, -5:-3]\n",
    "        x_data_test = xy_data[2700:, :-11]\n",
    "        y_data_test = xy_data[2700:, -5:-3]\n",
    "        forest_reg = RandomForestRegressor(random_state=0, n_estimators=100, n_jobs=-1)\n",
    "        forest_reg.fit(x_data_training, y_data_training)\n",
    "        y_pred=forest_reg.predict(x_data_test)\n",
    "        mse=mean_squared_error(y_data_test, y_pred, multioutput='uniform_average')\n",
    "        r2=r2_score(y_data_test, y_pred, multioutput='variance_weighted')\n",
    "        result=np.array([mse,r2]).reshape(-1,2)    \n",
    "        np.savetxt('file path' ,result, delimiter=',',fmt='%4f')\n",
    "\n",
    "def rf_regression_ternary_model(file_name,file_num):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    tf.set_random_seed(777)  # for reproducibility\n",
    "    np.random.seed(777)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        xy = np.loadtxt(file_name, delimiter=',', dtype=np.float32)\n",
    "        xy_data=xy.reshape(-1,4012) \n",
    "        x_data_training = xy_data[:9000, :-11]\n",
    "        y_data_training = xy_data[:9000, -5:-2]\n",
    "        x_data_test = xy_data[9000:, :-11]\n",
    "        y_data_test = xy_data[9000:, -5:-2]\n",
    "        forest_reg = RandomForestRegressor(random_state=0, n_estimators=100, n_jobs=-1)\n",
    "        forest_reg.fit(x_data_training, y_data_training)\n",
    "        y_pred=forest_reg.predict(x_data_test)\n",
    "        mse=mean_squared_error(y_data_test, y_pred, multioutput='uniform_average')\n",
    "        r2=r2_score(y_data_test, y_pred, multioutput='variance_weighted')\n",
    "        result=np.array([mse,r2]).reshape(-1,2)    \n",
    "        np.savetxt('file path' ,result, delimiter=',',fmt='%4f')\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support_vector_machine_regression   \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "def svm_regression_binary_model(file_name,file_num):\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.multioutput import MultiOutputRegressor\n",
    "    start = time.time() \n",
    "    tf.set_random_seed(777)  # for reproducibility\n",
    "    np.random.seed(777)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        xy = np.loadtxt(file_name, delimiter=',', dtype=np.float32)\n",
    "        xy_data=xy.reshape(-1,4012) \n",
    "        x_data_training = xy_data[:2700, :-11]\n",
    "        y_data_training = xy_data[:2700, -5:-3]\n",
    "        x_data_test = xy_data[2700:, :-11]\n",
    "        y_data_test = xy_data[2700:, -5:-3]\n",
    "        svr_reg = MultiOutputRegressor(SVR(gamma='scale'),n_jobs=-1)\n",
    "        svr_reg.fit(x_data_training, y_data_training)\n",
    "        y_pred=svr_reg.predict(x_data_test)\n",
    "        mse=mean_squared_error(y_data_test, y_pred, multioutput='uniform_average')\n",
    "        r2=r2_score(y_data_test, y_pred, multioutput='variance_weighted')\n",
    "        result=np.array([mse,r2]).reshape(-1,2)    \n",
    "        np.savetxt('file path' ,result, delimiter=',',fmt='%4f')\n",
    "\n",
    "def svm_regression_ternary_model(file_name,file_num):\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.multioutput import MultiOutputRegressor\n",
    "    tf.set_random_seed(777)  # for reproducibility\n",
    "    np.random.seed(777)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        xy = np.loadtxt(file_name, delimiter=',', dtype=np.float32)\n",
    "        xy_data=xy.reshape(-1,4012) \n",
    "        x_data_training = xy_data[:9000, :-11]\n",
    "        y_data_training = xy_data[:9000, -5:-2]\n",
    "        x_data_test = xy_data[9000:, :-11]\n",
    "        y_data_test = xy_data[9000:, -5:-2]\n",
    "        svr_reg = MultiOutputRegressor(SVR(gamma='scale'),n_jobs=-1)\n",
    "        svr_reg.fit(x_data_training, y_data_training)\n",
    "        y_pred=svr_reg.predict(x_data_test)\n",
    "        mse=mean_squared_error(y_data_test, y_pred, multioutput='uniform_average')\n",
    "        r2=r2_score(y_data_test, y_pred, multioutput='variance_weighted')\n",
    "        result=np.array([mse,r2]).reshape(-1,2)\n",
    "        np.savetxt('file path' ,result, delimiter=',',fmt='%4f')        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time, math\n",
    "\n",
    "for i in range(21,1561):\n",
    "    if i<231:\n",
    "        regression_binary_model('file path', i)\n",
    "#         knn_regression_binary_model('file path', i)\n",
    "#         rf_regression_binary_model('file path', i)\n",
    "#         svm_regression_binary_model('file path', i)\n",
    "        \n",
    "    else:\n",
    "        regression_ternary_model('file path', i)   \n",
    "#         knn_regression_ternary_model('file path', i)\n",
    "#         rf_regression_ternary_model('file path', i)\n",
    "#         svm_regression_ternary_model('file path', i)\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
