{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "\n",
    "#================================================================== Data Split ======================================================\n",
    "\n",
    "import numpy as np\n",
    "import random, shutil, glob\n",
    "\n",
    "num_validation_file = 100\n",
    "test_validation_file = 100\n",
    "File_List = glob.glob('file path_total')\n",
    "list_index = list(np.arange(0, len(File_List), 1))\n",
    "random_index = random.sample(list_index,len(File_List))\n",
    "for i in range(len(random_index)):\n",
    "    random_index_ = random_index[i]\n",
    "    print(random_index_)\n",
    "#==========================================================================VALIDATION SET==========================================\n",
    "    if i < num_validation_file:\n",
    "        shutil.copy('file path_total' %random_index_, 'file path_Validation' %random_index_)\n",
    "#==========================================================================Test SET==========================================\n",
    "    if num_validation_file <= i and num_validation_file + test_validation_file > i:\n",
    "        shutil.copy('file path_total' %random_index_, 'file path_Test' %random_index_)\n",
    "#==========================================================================Training SET==========================================    \n",
    "    if num_validation_file + test_validation_file <= i:\n",
    "        shutil.copy('file path_total' %random_index_, 'file path_Training' %random_index_)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Classification\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "tf.set_random_seed(777)  \n",
    "\n",
    "learning_rate = 0.001\n",
    "num_files = 700\n",
    "vld_num_files = 100\n",
    "te_num_files = 100\n",
    "X_length = 4012\n",
    "batch_size_ = 1000\n",
    "n_classes = 21\n",
    "epochs = 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer([('file path_Training' % i) for i in range(num_files)],  \n",
    "                                                    shuffle=True, name='filename_queue')\n",
    "    reader = tf.TextLineReader()\n",
    "    key, value = reader.read(filename_queue)\n",
    "    record_defaults = [[0] for _ in range(X_length)]\n",
    "    record_defaults = [tf.constant([0], dtype=tf.float32) for _ in range(X_length)]\n",
    "    xy_data = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    xy_data = tf.stack(xy_data)\n",
    "    y_1=tf.cast(xy_data[-8], tf.int32)\n",
    "    y_2=tf.cast(xy_data[-7], tf.int32)\n",
    "    y_3=tf.cast(xy_data[-6], tf.int32)\n",
    "    y_1=tf.one_hot(y_1, n_classes)\n",
    "    y_2=tf.one_hot(y_2, n_classes)\n",
    "    y_3=tf.one_hot(y_3, n_classes)\n",
    "    y_data = y_1 + y_2 + y_3\n",
    "    y_data_ = y_1*xy_data[-4] + y_2*xy_data[-3] + y_3*xy_data[-2]\n",
    "    y_data = tf.to_float(y_data)\n",
    "    y_data_ = tf.to_float(y_data_)\n",
    "    X_train, y_train,  y_train_, y_train_p, y_train_ind = tf.train.batch([xy_data[:-11], y_data, y_data_, xy_data[-5:-2],\n",
    "                                                                xy_data[-1:]], batch_size = batch_size_)    \n",
    "    \n",
    "#==========================================================================VALIDATION SET==========================================\n",
    "\n",
    "    filename_queue_vld = tf.train.string_input_producer([('file path_Validation' % i) for i in range(vld_num_files)], \n",
    "                                                        shuffle=True, name='filename_queue')\n",
    "    reader_vld = tf.TextLineReader()\n",
    "    key, value_vld = reader_vld.read(filename_queue)\n",
    "    record_defaults = [[0] for _ in range(X_length)]\n",
    "    record_defaults = [tf.constant([0], dtype=tf.float32) for _ in range(X_length)]\n",
    "    xy_data_vld = tf.decode_csv(value_vld, record_defaults=record_defaults)\n",
    "    xy_data_vld = tf.stack(xy_data_vld)\n",
    "    y_1_vld=tf.cast(xy_data_vld[-8], tf.int32)\n",
    "    y_2_vld=tf.cast(xy_data_vld[-7], tf.int32)\n",
    "    y_3_vld=tf.cast(xy_data_vld[-6], tf.int32)\n",
    "    y_1_vld=tf.one_hot(y_1_vld, n_classes)\n",
    "    y_2_vld=tf.one_hot(y_2_vld, n_classes)\n",
    "    y_3_vld=tf.one_hot(y_3_vld, n_classes)\n",
    "    y_data_vld = y_1_vld + y_2_vld + y_3_vld\n",
    "    y_data_vld_ = y_1_vld*xy_data_vld[-5] + y_2_vld*xy_data_vld[-4] + y_3_vld*xy_data_vld[-3]\n",
    "    y_data_vld = tf.to_float(y_data_vld)\n",
    "    y_data_vld_ = tf.to_float(y_data_vld_)\n",
    "    X_vld, y_vld, y_vld_, y_vld_p, y_vld_ind = tf.train.batch([xy_data_vld[:-11], y_data_vld, y_data_vld_, xy_data_vld[-5:-2], \n",
    "                                                       xy_data_vld[-1:]], batch_size = batch_size_)    \n",
    "    \n",
    "#==========================================================================TEST SET==========================================    \n",
    "        \n",
    "    filename_queue = tf.train.string_input_producer([('file path_Test' % i) for i in range(te_num_files)], \n",
    "                                                    shuffle=True, name='filename_queue')\n",
    "    reader_te = tf.TextLineReader()\n",
    "    key, value_te = reader_te.read(filename_queue)\n",
    "    record_defaults = [[0] for _ in range(X_length)]\n",
    "    record_defaults = [tf.constant([0], dtype=tf.float32) for _ in range(X_length)]\n",
    "    xy_data_te = tf.decode_csv(value_te, record_defaults=record_defaults)\n",
    "    xy_data_te = tf.stack(xy_data_te)\n",
    "    y_1_te=tf.cast(xy_data_te[-8], tf.int32)\n",
    "    y_2_te=tf.cast(xy_data_te[-7], tf.int32)\n",
    "    y_3_te=tf.cast(xy_data_te[-6], tf.int32)\n",
    "    y_1_te=tf.one_hot(y_1_te, n_classes)\n",
    "    y_2_te=tf.one_hot(y_2_te, n_classes)\n",
    "    y_3_te=tf.one_hot(y_3_te, n_classes)\n",
    "    y_data_te = y_1_te + y_2_te + y_3_te\n",
    "    y_data_te_ = y_1_te*xy_data[-5] + y_2_te*xy_data[-4] + y_3_te*xy_data[-3]\n",
    "    y_data_te = tf.to_float(y_data_te)\n",
    "    y_data_te_ = tf.to_float(y_data_te_)\n",
    "    X_Test, y_Test,  y_Test_, y_Test_p, y_Test_ind = tf.train.batch([xy_data_te[:-11], y_data_te, y_data_te_, xy_data_te[-5:-2],\n",
    "                                                                xy_data_te[-1:]], batch_size = batch_size_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, 4001, 1], name = 'inputs')\n",
    "    labels_1 = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_1')\n",
    "    logit_num = tf.placeholder(tf.int32, [None, 3], name = 'logits_Top_3')\n",
    "    label_num = tf.placeholder(tf.int32, [None, 3], name = 'labels_Top_3')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "    \n",
    "#   model architecture(CNN_2)\n",
    "    '''\n",
    "    \n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=15, strides=2,padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=2, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=10, strides=3, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=3, strides=3, padding='same')\n",
    "    flat = tf.reshape(max_pool_2, (-1, 112*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2000, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 500,  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_2 = tf.layers.dense(logits_, n_classes, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    '''    \n",
    "\n",
    "#   model architecture(CNN_3)\n",
    "\n",
    "    '''\n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=20, strides=1,padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=3, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=15, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=3, strides=3, padding='same')\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=10, strides=2, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "    flat = tf.reshape(max_pool_3, (-1, 112*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2500, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 1000,  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_2 = tf.layers.dense(logits_, n_classes, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    '''\n",
    "    \n",
    "#   model architecture(CNN_4)\n",
    "    '''\n",
    "    \n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=25, strides=1,padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=2, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=20, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=3, strides=2, padding='same')\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=15, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "    conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=64, kernel_size=10, strides=2, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
    "    flat = tf.reshape(max_pool_4, (-1, 126*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2500, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 1000,  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_2 = tf.layers.dense(logits_, n_classes, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    '''\n",
    "#   model architecture(CNN_5)\n",
    "    '''\n",
    "    \n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=30, strides=1,padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=2, padding='same')\n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=25, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=3, strides=2, padding='same')\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=20, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "    conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=64, kernel_size=15, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
    "    conv5 = tf.layers.conv1d(inputs=max_pool_4, filters=64, kernel_size=10, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_5 = tf.layers.max_pooling1d(inputs=conv5, pool_size=2, strides=2, padding='same')\n",
    "    flat = tf.reshape(max_pool_5, (-1, 126*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2500, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 1000,  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_2 = tf.layers.dense(logits_, n_classes, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    '''\n",
    "\n",
    "#   model architecture(CNN_6)\n",
    "    \n",
    "\n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=64, kernel_size=35, strides=1,padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=3, strides=2, padding='same')\n",
    "    \n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=30, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=3, strides=2, padding='same')\n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=64, kernel_size=25, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "    conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=64, kernel_size=20, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
    "    conv5 = tf.layers.conv1d(inputs=max_pool_4, filters=64, kernel_size=15, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_5 = tf.layers.max_pooling1d(inputs=conv5, pool_size=2, strides=2, padding='same')\n",
    "    conv6 = tf.layers.conv1d(inputs=max_pool_5, filters=64, kernel_size=10, strides=1, padding='same',\n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer(), activation = tf.nn.relu)\n",
    "    max_pool_6 = tf.layers.max_pooling1d(inputs=conv6, pool_size=2, strides=2, padding='same')\n",
    "    flat = tf.reshape(max_pool_6, (-1, 63*64))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    logits = tf.layers.dense(flat, 2500, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits= tf.nn.dropout(logits, keep_prob=keep_prob_)\n",
    "    logits_ = tf.layers.dense(logits, 1000,  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    logits_= tf.nn.dropout(logits_, keep_prob=keep_prob_)\n",
    "    logits_2 = tf.layers.dense(logits_, n_classes, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_2, labels=labels_1))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n",
    "    correct_pred = tf.equal(logit_num, label_num)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Classification\n",
    "\n",
    "################################################################## Start Training ##########################################################\n",
    "\n",
    "epochs= 20\n",
    "before_gen_num= 0\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        train_result_list=[]\n",
    "        train_result_list_1=[]\n",
    "        \n",
    "        validation_result_list=[]\n",
    "        validation_result_list_1=[]\n",
    "        \n",
    "        iteration = 1\n",
    "        \n",
    "        if e+before_gen_num ==0:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "        else:\n",
    "            saver.restore(sess, 'file path')\n",
    "            \n",
    "            \n",
    "        for i in  range(70):     \n",
    "            sum_ = 0\n",
    "            sum_1 = 0\n",
    "            X_tr, y_tr, y_tr_, y_tr_p, y_ind = sess.run([X_train, y_train, y_train_, y_train_p, y_train_ind])\n",
    "            X_tr= np.reshape(X_tr, (-1, 4001, 1))\n",
    "            feed = {inputs_ : X_tr, labels_1 : y_tr, keep_prob_ : 0.5, learning_rate_ : learning_rate}\n",
    "            loss, _ , logit = sess.run([cost, optimizer, logits_2], feed_dict = feed)\n",
    "            y_lab = np.empty([batch_size_, 3])\n",
    "            y_logit = np.empty([batch_size_, 3])\n",
    "            for i in range(batch_size_):\n",
    "                if y_ind[i,0] == 2:\n",
    "                    y_lab[i]=np.argsort(y_tr_[i])[-3:]\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    y_logit[i]=np.argsort(logit[i])[-3:]\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1= sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1\n",
    "                elif y_ind[i,0] == 1:\n",
    "                    z=np.argsort(y_tr_[i])[-2:]\n",
    "                    y_lab[i]=np.append(z, [-1])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-2:]\n",
    "                    y_logit[i]=np.append(z_, [-1])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1 = sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1         \n",
    "                elif y_ind[i,0] == 0:\n",
    "                    z=np.argsort(y_tr_[i])[-1:]\n",
    "                    y_lab[i]=np.append(z, [-1,-2])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-1:]\n",
    "                    y_logit[i]=np.append(z_, [-1,-2])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1 = sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1\n",
    "            acc_=sum_/len(y_lab)\n",
    "            acc_1=sum_1/(len(y_lab)*y_lab.shape[1])\n",
    "            \n",
    "            result=['Epoch:' , e, 'iteration:', iteration, 'Train_loss:', loss, 'Train_sample_acc:', acc_]\n",
    "            result_1=['Epoch:' , e, 'iteration:', iteration, 'Train_loss:', loss, 'Train_constituent_acc:', acc_1]\n",
    "            \n",
    "            train_result_list.append(result)\n",
    "            train_result_list_1.append(result_1)\n",
    "            \n",
    "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                   \"Iteration: {:d}\".format(iteration),\n",
    "                   \"Train loss: {:6f}\".format(loss),\n",
    "                   \"Train sample-based acc: {:.6f}\".format(acc_),\n",
    "                   \"Train constituent-based acc_1: {:.6f}\".format(acc_1))\n",
    "            \n",
    "###================================================================ VALIDATION =====================================            \n",
    "            if (iteration %5 == 0):\n",
    "                sum_ = 0\n",
    "                sum_1 = 0\n",
    "                X_vd, y_vd, y_vd_, y_vd_p, y_ind_vd = sess.run([X_vld, y_vld, y_vld_, y_vld_p, y_vld_ind])\n",
    "                X_vd= np.reshape(X_vd, (-1, 4001, 1))\n",
    "                feed = {inputs_ : X_vd, labels_1 : y_vd, keep_prob_ : 1.0, learning_rate_ : learning_rate}\n",
    "                loss_vd,  logit_vd = sess.run([cost, logits_2], feed_dict = feed)\n",
    "                y_lab_vd = np.empty([batch_size_, 3])\n",
    "                y_logit_vd = np.empty([batch_size_, 3])\n",
    "                for i in range(batch_size_):\n",
    "                    if y_ind_vd[i,0] == 2:\n",
    "                        y_lab_vd[i]=np.argsort(y_vd_[i])[-3:]\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])\n",
    "                        y_logit_vd[i]=np.argsort(logit_vd[i])[-3:]\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                        b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                        sum_1= sum_1+b1\n",
    "                        if b1 == 3:\n",
    "                            sum_+=1\n",
    "                    elif y_ind_vd[i,0] == 1:\n",
    "                        z=np.argsort(y_vd_[i])[-2:]\n",
    "                        y_lab_vd[i]=np.append(z, [-1])\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])                    \n",
    "                        z_=np.argsort(logit_vd[i])[-2:]\n",
    "                        y_logit_vd[i]=np.append(z_, [-1])\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                        b1 = len(list(set(y_lab_vd[i]).intersection(y_logit_vd[i])))\n",
    "                        sum_1= sum_1+b1\n",
    "                        if b1 == 3:\n",
    "                            sum_+=1\n",
    "                    elif y_ind_vd[i,0] == 0:\n",
    "                        z=np.argsort(y_vd_[i])[-1:]\n",
    "                        y_lab_vd[i]=np.append(z, [-1,-2])\n",
    "                        y_lab_vd[i]=np.sort(y_lab_vd[i])\n",
    "                        z_=np.argsort(logit_vd[i])[-1:]\n",
    "                        y_logit_vd[i]=np.append(z_, [-1,-2])\n",
    "                        y_logit_vd[i]=np.sort(y_logit_vd[i])\n",
    "                        b1 = len(list(set(y_lab_vd[i]).intersection(y_logit_vd[i])))\n",
    "                        sum_1= sum_1+b1\n",
    "                        if b1 == 3:\n",
    "                            sum_+=1\n",
    "                acc_vd_=sum_/len(y_lab_vd)            \n",
    "                acc_vd_1=sum_1/(len(y_lab_vd)*y_lab_vd.shape[1])\n",
    "                result_validation=['Epoch:' , e, 'iteration:', iteration, 'Validation_loss:', loss_vd, 'Validation_sample_acc:', acc_vd_]\n",
    "                result_validation_1=['Epoch:' , e, 'iteration:', iteration, 'Validation_loss:', loss_vd, 'Validation_constituent_acc:', acc_vd_1]\n",
    "                validation_result_list.append(result_validation)\n",
    "                validation_result_list_1.append(result_validation_1)\n",
    "                \n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                        \"Iteration: {:d}\".format(iteration),\n",
    "                        \"Validation loss: {:6f}\".format(loss_vd),\n",
    "                        \"Validation sample-based acc: {:.6f}\".format(acc_vd_),\n",
    "                         \"Validation constituent-based acc_1: {:.6f}\".format(acc_vd_1))\n",
    "            iteration += 1\n",
    "        saver.save(sess,'file path')\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "####################################################################End of Training ##########################################################\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###================================================================ Test Code =====================================         \n",
    "\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_acc_1=[]\n",
    "iteration = 1\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, 'file path')\n",
    "    \n",
    "    for i in  range(10):\n",
    "        sum_ = 0\n",
    "        sum_1 = 0\n",
    "        X_te, y_te, y_te_, y_te_p, y_ind = sess.run([X_Test, y_Test, y_Test_, y_Test_p, y_Test_ind])\n",
    "        X_te_2 = X_te\n",
    "        X_te = np.reshape(X_te, (-1, 4001, 1))\n",
    "        feed = {inputs_ : X_te, labels_1 : y_te, keep_prob_ : 1.0, learning_rate_ : learning_rate}\n",
    "        batch_loss, logit = sess.run([cost, logits_2], feed_dict = feed)            \n",
    "        y_lab = np.empty([batch_size_, 3])\n",
    "        y_logit = np.empty([batch_size_, 3])\n",
    "        ann_result = np.empty([batch_size_, 2])\n",
    "        for i in range(batch_size_):\n",
    "            if y_ind[i,0] == 2:\n",
    "                y_lab[i]=np.argsort(y_te_[i])[-3:]\n",
    "                y_lab[i]=np.sort(y_lab[i])\n",
    "                y_logit[i]=np.argsort(logit[i])[-3:]\n",
    "                y_logit[i]=np.sort(y_logit[i])\n",
    "                b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                sum_1= sum_1+b1\n",
    "                if b1 == 3:\n",
    "                    sum_+=1\n",
    "            elif y_ind[i,0] == 1:\n",
    "                z=np.argsort(y_te_[i])[-2:]\n",
    "                y_lab[i]=np.append(z, [-1])\n",
    "                y_lab[i]=np.sort(y_lab[i])\n",
    "                z_=np.argsort(logit[i])[-2:]\n",
    "                y_logit[i]=np.append(z_, [-1])\n",
    "                y_logit[i]=np.sort(y_logit[i])\n",
    "                y_te_p[i] = np.sort(y_te_p[i])\n",
    "                b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                sum_1= sum_1+b1\n",
    "                if b1 == 3:\n",
    "                    sum_+=1\n",
    "            elif y_ind[i,0] == 0:\n",
    "                z=np.argsort(y_te_[i])[-1:]\n",
    "                y_lab[i]=np.append(z, [-1,-2])\n",
    "                y_lab[i]=np.sort(y_lab[i])\n",
    "                z_=np.argsort(logit[i])[-1:]\n",
    "                y_logit[i]=np.append(z_, [-1,-2])\n",
    "                y_logit[i]=np.sort(y_logit[i])\n",
    "                y_te_p[i] = np.sort(y_te_p[i])\n",
    "                b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                sum_1= sum_1+b1\n",
    "                if b1 == 3:\n",
    "                    sum_+=1\n",
    "            else:\n",
    "                print('Something Wrong happened!!!')\n",
    "               \n",
    "        '''\n",
    "        The following code can also be used for test.\n",
    "        The number of positive nodes on the output layer before the sigmoid-cross-entropy loss function can be used as being indicative of whether \n",
    "        the sample to be tested is unary, binary or ternary.\n",
    "        k is the adjustable threshold parameter. Normally k=0, then almost the same test accuracy as the original code above can be obtained.\n",
    "        If k>0, the test accuracy will improve especially for real-word-data test.\n",
    "        '''        \n",
    "        \n",
    "        '''\n",
    "        k=1\n",
    "        if k!=0:\n",
    "            sum_ = 0\n",
    "            sum_1 = 0\n",
    "            for i in range(batch_size_):\n",
    "                if np.count_nonzero(logit[i] > 1.0) >= 3:\n",
    "                    y_lab[i]=np.argsort(y_te_[i])[-3:]\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    y_logit[i]=np.argsort(logit[i])[-3:]\n",
    "                    y_logit[i]=np.sort(y_logit[i])                     \n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1= sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1 \n",
    "                elif np.count_nonzero(logit[i] > 1.0) == 2:\n",
    "                    z=np.argsort(y_te_[i])[-2:]\n",
    "                    y_lab[i]=np.append(z, [-1])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-2:]\n",
    "                    y_logit[i]=np.append(z_, [-1])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1= sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1\n",
    "                elif np.count_nonzero(logit[i] > 1.0) == 1:\n",
    "                    z=np.argsort(y_te_[i])[-1:]\n",
    "                    y_lab[i]=np.append(z, [-1,-2])\n",
    "                    y_lab[i]=np.sort(y_lab[i])\n",
    "                    z_=np.argsort(logit[i])[-1:]\n",
    "                    y_logit[i]=np.append(z_, [-1,-2])\n",
    "                    y_logit[i]=np.sort(y_logit[i])\n",
    "                    b1 = len(list(set(y_lab[i]).intersection(y_logit[i])))\n",
    "                    sum_1= sum_1+b1\n",
    "                    if b1 == 3:\n",
    "                        sum_+=1\n",
    "                else:\n",
    "                    print('Something Wrong happened!!!')\n",
    "                    print(np.sort(logit[i])[::-1])\n",
    "                    '''\n",
    "                \n",
    "        acc_te_=sum_/len(y_lab)            \n",
    "        acc_te_1=sum_1/(len(y_lab)*y_lab.shape[1])\n",
    "        test_loss.append(batch_loss)\n",
    "        test_acc.append(acc_te_)\n",
    "        test_acc_1.append(acc_te_1)\n",
    "        \n",
    "        print(\"Iteration: {:d}\".format(iteration),\n",
    "                  \"Test loss: {:6f}\".format(batch_loss),\n",
    "                  \"Test sample-based acc: {:.6f}\".format(acc_te_),\n",
    "                  \"Test constituent-based acc_1: {:.6f}\".format(acc_te_1))\n",
    "        iteration += 1\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)        \n",
    "    print(\"Average Test loss: {:.6f}\".format(np.mean(test_loss)))\n",
    "    print(\"Average Test sample_based accuracy: {:.6f}\".format(np.mean(test_acc)))\n",
    "    print(\"Average Test constituent_based accuracy: {:.6f}\".format(np.mean(test_acc_1)))\n",
    "    \n",
    "####################################################################End of Test ##########################################################    \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
